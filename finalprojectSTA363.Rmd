---
title: "Predicting For The Weight of Stone Flakes Found in Portugal"
author: "Owen Soccorso"
date: "2024-12-12"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

\newpage
\tableofcontents
\newpage

# Abstract

Creating a model or algorithm that can accurately predict the weight of stone flakes is important for archaeological research, as oftentimes data on technology or processes is lost to the past. Accurately predicting flake weight can help researchers determine the technologies and cultural practices used to create the flakes.  The stone flakes analyzed come from an archaeological site in Portugal, and were collected by archaeological researchers in the Wake Forest Archaeology Department. In this report, we discuss how to estimate flake weight using three different methods; KNN, regression tree, and Elastic Net. We explain the steps and processes for each method, and compare their predictive abilities and accuracy. Based on our analysis, we conclude that KNN with 9 neighbors is the best method to predict the weight of stone flakes. In the following research paper we further discuss our findings and research limitations. 

# Section 1: Introduction

The goal of this research is to accurately predict the weight of stone flakes created by ancient peoples in Portugal. Stone flakes are sharp, small pieces of stone that are chipped off a larger stone, the core, by hitting the core. The weight of the flakes is related to the technology used to produce them, and so if we can accurately predict weight, we may be able to make informed hypotheses about the technology and processes used by people of this past culture. 

The data set we will use to predict the weight of stone flakes contains 657 rows, and 23 columns. In addition to our response variable of weight in grams, we also have information on maximum length in mm, width at the midpoint in mm, thickness at the midpoint in mm, platform depth in mm, platform width in mm, and the percent cortical. Cortex is the outside material of a stone, and flakes are typically characterized by how much outer material is present on them.

Additional information on the rest of the columns can be found in the excel spreadsheet containing the data. 

# Section 2: Data Cleaning and EDA

Before working with our data to make predictions, we first need to examine our data and clean it up. The first thing we do is look at a visual representation of our data. Here in Figure 1 we can see all of our rows and columns in the data set. The columns are color-coded by the type of variable they are, and the gray spots in the plot show where there is missing data. Since most of the rows do not have data on exterior platform angle, we are going to remove that column from our data set.

We are also going to remove the columns of flake depth, tracking number, both unit coordinate columns, level, raw material type, and both columns containing comments. This data can be useful in some kinds of analysis, but since we know all of these stone flakes came from the same site, they are not important for this analysis. In addition, we will remove the columns denoting whether the flake was rolled or polished, cross-section angle, as well as platform type as these are also not relevant for this analysis. 


```{r}
# This line of code uploads the data from the computer file into R
LithicsData <- read.csv("Olival Grande data.csv")
```

```{r}
# These lines of code load the packages we need to visualize our data.
library(visdat)
library(ggplot2)
# This line of code gives us a visual representation of our data, as well as a title for the plot.
vis_dat(LithicsData) + labs(title="Figure 1:")
```

```{r}
# This line of code removes the features in the data that we will not be using in this analysis
LithicsData_clean <- LithicsData[,-c(1:6,9,16,20,21,22,23)]
```

After removing the unnecessary columns, we will run a Complete Case Analysis on the data, which will remove any rows with missing data. Doing this takes our data set from 657 rows to 654 rows, and leaves us with 11 columns.

```{r}
# This line runs a Complete Case Analysis on the data, and stores the new data as "Lithics."
Lithics <- na.omit(LithicsData_clean)
# This line divides the column of "% cortical" by 100. The original dataset did not have this column represented as decimals, but rather whole numbers going from 0-100. For our purposes, it makes more sense for these numbers to be scaled down into their decimal representation.
Lithics$X..cortical <- Lithics$X..cortical/100
```


In order to assess our data, we created a correlogram showing the correlation between all of our features.

```{r}
# This line of code loads the package, "corrplot" we need to create a correlogram
library(corrplot)
# These lines of code create the correlogram. The method "pie" specifies that we want pie chart representations of correlation, and the type "upper" that we only want the top half of the correlogram.
C <- cor(Lithics)

corrplot(C,method="pie",type="upper")
title("Figure 2:", line=3.5, cex.main=.8)
```

Figure 2 shows our correlogram, with correlations represented as pie charts. We can see there there are no perfectly correlated features, and that the features of Abraded platform, lipped platform, and overpass termination don't have a strong correlation with any of the other features or our response variable. 

We also examine the range in our response variable, stone flake weight, which is 99.53 grams. This important to know, because it will give us context when we examine predictive accuracy of our methods. 

```{r}
# This line of code copies the Lithics data. We did this so that we can make adjustments to the dataset without changing the original
LithicsRT <- Lithics

# The following lines of code change the features of burned, abraded platform, lipped platform, and overpass termination to factors. In the original dataset they were coded as dummy indicator variables, which will be useful for KNN and Elastic Net, but it will be better for the regression tree for them to be factors. 
LithicsRT$Burned. <- as.factor(LithicsRT$Burned.)
LithicsRT$Abraded.Plat <- as.factor(LithicsRT$Abraded.Plat)
LithicsRT$Lipped.Plat <- as.factor(LithicsRT$Lipped.Plat)
LithicsRT$Overpass.Termination <- as.factor(LithicsRT$Overpass.Termination)
```

# Section 3: Method 1

## Section 3.1: Introduction

The first method we will be using to predict the weight of the stone flakes is KNN. KNN is an appropriate choice to predict the weight of the flakes because the flakes vary a lot in size. KNN allows for prediction of a response for a flake based on only the flakes that have the most similar features. KNN can do this because it is an algorithm, not a model. This means that we don't have to worry about the shape of our data; it can follow any pattern, or no pattern, and KNN will still work.

## Section 3.2: Method

In this section, we will use KNN to predict the weight of the stone flakes. The first step to KNN is choosing the k, which is how many neighbors, or rows in the training set, you want to use to estimate for a row in the validation set. After you have chosen k, for every row in the validation set, KNN finds the k nearest rows to that one, and averages the response variables in the k nearest rows to find an estimate for the response variable in the validation set. There are a few ways to determine which rows are closest to the row in the validation set we are estimating for, but in this case we are using Euclidean Distance. 

KNN relies on the user deciding the value of k, but sometimes it isn't clear what that value should be. A general rule of thumb is to use the square root the number of rows you have, which in this case is around 25. However, in order to get the best predictive accuracy we can, we want to run a 10-Fold CV for all options of k 3-41. We will then evaluate which k resulted in the lowest RMSE, and choose that for the algorithm. 

## Section 3.3: Results


```{r}

# This chunk of code teaches R this function, which is used to compute RMSE. Essentially, this function breaks down the RMSE part-by-part to compute it. 

compute_RMSE <- function(truth, predictions){

  #Part 1
  part1 <- truth - predictions
  
  #Part 2
  part2 <- part1^2
  
  #Part 3: RSS 
  part3 <- sum(part2)
  
  #Part4: MSE
  part4 <- 1/length(predictions)*part3
  
  # Part 5:RMSE
  sqrt(part4)
}
```

```{r}
# This line loads the package we need in order to run the KNN function
library(caret)
```

```{r}
# This line repeats the numbers one to ten 66 times, and saves all those numbers as "tickets_knn"
tickets_knn <- rep(1:10,66)
# This line sets a random seed, and ensures that every time the code is run, the same random sampling occurs
set.seed(101)
# This line scrambles up all 66 of the copies of 1-10, and saves the mixed-up numbers as "folds_knn." Since our data set only has 654 rows, we only need to sample 654 of the 66 1-10 copies. 
folds_knn <-sample(tickets_knn,654,replace=FALSE)
# We create a data frame to hold potential values of k for the 10-fold CV we are about to run for KNN, and the resultant RMSEs.
# Since we don't have the values of k or our RMSEs yet, for now we fill all 39 rows with "NA" in both columns. We know we need 39 rows, because we are testing values of k from 3 to 41.
# We save our data frame as "RMSE_data" so that it is easy to reference later.
RMSE_data <- data.frame("k" = rep(NA,39), "RMSE" = rep(NA,39))
```

```{r}
# This line starts a for loop.
# Since we want to test values of k from 3-41, we want our loop to run through all of those values.
for(i in 3:41){
  # We create a data frame to hold our estimated flake weights from the cross validation we are about to do, and the true flake weights
  # We input our true flake weights into the data frame, but since we don't have our estimates yet, for now we fill all 654 rows with "NA" in that column.
  # We save our data frame as "predictions_knn" so that it is easy to reference later.
  predictions_knn <-data.frame("Estimated_Weight" = rep(NA,654),"True_Weight"=Lithics$Weight)
  # This line starts a for loop. Since it is already inside a for loop, it is nested.
  # Since we want to run a 10-fold CV, we need the loop to run 10 times, with f=(1-10).
  for(f in 1:10){
    # In this line, we take the scrambled up 654 1-10 numbers, and select those that are the same number at the loop we are in
  # All of the numbers that match what loop we are in get stores as "infold_KNN"
    infold_KNN <- which(folds_knn == f)
    # All of the numbers that are in "infold_KNN" are taken out of rows of our big data set.
  # This means that all the rows corresponding to the numbers in "infold_KNN" are deleted. This smaller data set is saved as "newtrain_KNN"
    newtrain_KNN <- Lithics[-infold_KNN,]
    # The same thing, but opposite, occurs here. Only the rows corresponding to the numbers in "infold_KNN" are kept in the big data set. These rows are saved as "validation_KNN"
    validation_KNN <- Lithics[infold_KNN,]
    # Next, we need to run KNN. We input our training data, minus the feature we are trying to predict for (flake weight), and then the flake weight in our training data. We also specify our k, which is i since we are running a loop. This result is saved as "results." This essentially trains the algorithm, which we can then use to predict for our validation data.
    results <- knnreg( (newtrain_KNN[,-1]) , newtrain_KNN[,1], k = i)
    # Then, we run KNN using Euclidean distance. We take the "results" we had last time, and then also input "validation_KNN" without the flake weight feature. This will run KNN for flake weight using the training data, and whatever k corresponding to the loop we are on. This result is saved as "knnPred."
    knnPred <- predict(results, newdata = validation_KNN[,-1])
    # Finally, we save our predictions in the data frame we made, under the columns            "Estimated_Weight", and in the "infold_KNN" rows. This closes the inner for loop.
    predictions_knn[infold_KNN,"Estimated_Weight"] <- knnPred
  }
  # Now that we have done the 10-fold CV using KNN, we need to do a few more things before the loop repeats and the process is repeated for a new k. First, we need to store the k we have in the "RMSE_data" data frame. This value goes in the first column, and since our k's start with 3, the row each k needs to be in is k-2. 
  RMSE_data[i-2,1] <- i
 # Then, we need to compute the RMSE for our 10-fold CV with KNN. We input our true flake weight and our predicted flake weight values into the function, which is then stored in the second column, and the k-2 row. 
  RMSE_data[i-2,2] <- compute_RMSE(predictions_knn[,"True_Weight"],predictions_knn[,"Estimated_Weight"])
}

# This process will repeat for every value of k 3-41.

```

After running our 10-Fold CV for each k, we are left with a table displaying the RMSE for each k ran in KNN.  

```{r}

# This is the code used to create the table displaying the values of k and corresponding RMSEs. First we load the package, "xtable" we need to create the table, and then run the xtable function. In the function, we say that we want the first big column to contain half of our data, or rows 1-20 of both the k and 20-39 of both the k and RMSE columns. The double occurrence of row 20 will later be deleted in the text of the column. 

#library(xtable)
#xtable(cbind(RMSE_data[1:20,1:2], RMSE_data[20:39,1:2]))
```

\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
 k & RMSE & k & RMSE \\ 
  \hline \hline
  3 & 4.80 &   &\\ 
     4 & 4.56 &  23 & 5.09 \\ 
     5 & 4.48 &  24 & 5.10 \\ 
     6 & 4.37 &  25 & 5.14 \\ 
     7 & 4.48 &  26 & 5.20 \\ 
     8 & 4.41 &  27 & 5.25 \\ 
     9 & 4.35 &  28 & 5.29 \\ 
    10 & 4.39 &  29 & 5.29 \\ 
    11 & 4.46 &  30 & 5.34 \\ 
    12 & 4.44 &  31 & 5.35 \\ 
    13 & 4.52 &  32 & 5.37 \\ 
    14 & 4.61 &  33 & 5.42 \\ 
    15 & 4.69 &  34 & 5.43 \\ 
    16 & 4.75 &  35 & 5.46 \\ 
    17 & 4.79 &  36 & 5.50 \\ 
    18 & 4.85 &  37 & 5.52 \\ 
    19 & 4.90 &  38 & 5.54 \\ 
    20 & 4.96 &  39 & 5.58 \\ 
    21 & 5.00 &  40 & 5.61 \\ 
    22 & 5.04 &  41 & 5.64 \\ 
   \hline
\end{tabular}
\end{table}


Looking up at the table above, we can see that the lowest RMSE of 4.35 is found with a k of 9. This means that the best case scenario is that under KNN, our predictions for stone flake weight are on average 4.35 grams off from the true flake weights. 

# Section 4: Method 2

## Section 4.1: Introduction

The next method we will use to predict the weight of stone flakes is a regression tree. This method is appropriate because a regression tree can create many different response predictions based on the features, meaning that like KNN this method can deal with wide ranges in both the response variable and the features. Unlike KNN, a regression tree can perform selection on our features. This means that the tree can choose which features are important for prediction, and which are not.

## Section 4.2: Method

A regression tree works by starting with all the data, and taking the average of the response variable. This is the prediction for flake weight in the root node, or the start of the tree. From there, the tree searches through all the features and all the possible ways to split the features. The tree decides to grow when it has found a splitting rule that best minimizes the residual sum of squares, or RSS. RSS is a measure of how well our tree fits the real data. The tree will keep growing and creating smaller buckets, called leaves, with predictions until it has hit a stopping rule, which tells the tree when to stop growing. At the end of the tree, all the data has been split into the leaves, and in each leaf the predicted response is the mean of all the responses of the rows in the leaf. Essentially, the tree functions like a piece-wise function. 

To begin creating a regression tree, we first need to specify our stopping rules. We don't want our tree to grow forever, and so it is important to have some idea of when a tree should stop. For this tree, we want it to stop when each split no longer decreases R squared by at least .5%. We chose .5% because it is small enough that it will not hinder growth, but not so large that the tree will grow too big. 

## Section 4.3: Results

```{r}
# To use a regression tree, we need three packages. The first package allows us to grow our tree, while the second two help us to visualize the tree. 

library(rpart)
library(rattle)
library(rpart.plot)
```



```{r}
# This line of code creates our regression tree. First we specify what our response variable is and what features we want to use, then our data set, the method (anova specifies that we want a regression tree), and finally cp=0.005 is our stopping rule. 
TREE1 <- rpart(Weight ~., data = Lithics,
        method = "anova", cp=0.005)
# This line of code allows us to visualize our tree. 
fancyRpartPlot(TREE1, sub = "Figure 3")
```

Looking up at Figure 3, we can see our regression tree. This tree was built off of all the data, and each of the 12 leaves contains an estimate for flake weight based on the splitting rules and braches leading to the leaves. As we can see, using a regression tree with a stopping rule of R squared increasing by .5% results in a tree with 11 splitting rules. Of those splitting rules, only 3 features were used: Thickness at the midpoint, maximum length, and width at the midpoint. This means that our tree selected those features, and did not select any of the others as predictors for flake weight. 

After creating our regression tree, we need to assess its predictive accuracy. To do this, we will run a 10-Fold CV to see how well the regression tree can predict for stone flakes it wasn't built on. 

After running the 10-Fold CV,  we found an RMSE of 5.94. This means that on average, our model's predictions for flake weights are different from the true flake weights by 5.94 grams.

```{r}
# This line repeats the numbers one to ten 66 times, and saves all those numbers as "tickets_rt"
tickets_rt <- rep(1:10,66)
# This line sets a random seed, and ensures that every time the code is run, the same random sampling occurs
set.seed(101)
# This line scrambles up all 66 of the copies of 1-10, and saves the mixed-up numbers as "folds_rt." Since our data set only has 654 rows, we only need to sample 654 of the 66 1-10 copies. 
folds_rt <-sample(tickets_rt,654,replace=FALSE)
# Then, we create a data frame to hold our estimated flake weights from the cross validation we are about to do, and the true flake weights.
# We input our true flake weights into the data frame, but since we don't have our estimates yet, for now we fill all 654 rows with "NA" in that column.
# We save our data frame as "predictions_rt" so that it is easy to reference later.
predictions_rt <-data.frame("Estimated_Weight" = rep(NA,654),"True_Weight"=Lithics$Weight)
```

```{r}
# This line starts a for loop.
# Since we want to run 10-fold CV, we need to cycle through the loop ten times, which is why "f" goes from 1-10.
for(f in 1:10){
  # The next 3 lines are the exact same as the previous CV we ran. In summary, we select the numbers in our scrambled up sample that correspond to what fold we are on. We take the rows that correspond to the numbers and select them as our validation data, "validation_RT", and all the other rows are assigned to our training data,"newtrain_RT."
  infold_RT <- which(folds_rt == f)
  newtrain_RT <- LithicsRT[-infold_RT,]
  validation_RT <- LithicsRT[infold_RT,]
  # Now, we run a regression tree for flake weight based on all the other features, but we do so off of only the "newtrain_RT" data. We store this tree as "tree"
  tree <- rpart(Weight ~., data = newtrain_RT, method = "anova", cp=0.005)
  # Next, we use our tree, and ask it to run again, but with a different data set, which is the "validation_RT" we already created. This allows the tree to predict for values it hasn't yet seen. This result is stored as "YHatfolds."
  YHatfolds <- predict(tree, newdata = validation_RT)
  # Finally, the last step in our 10-fold CV is to save all our predictions, or "YHatfolds", into the data frame we made previously. 
  # We want to make sure our predictions are saved in the correct spot, so we specify the rows as "infold_RT" and the column as "Estimated_Weight."
  predictions_rt[infold_RT,"Estimated_Weight"] <- YHatfolds
  
  # This loop will run 10 times before it is complete. 
}
```

```{r}
# We use this line to compute the RMSE based on the 10 fold CV we just did with our regression tree.
# We input into the function our true values of flake weight, and our estimated values, and the function does all the computational work for us. 

#compute_RMSE(predictions_rt[,"True_Weight"],predictions_rt[,"Estimated_Weight"])
```


# Section 5: Method 3

## Section 5.1: Introduction

The final method we will be using to predict the weight of stone flakes is Elastic Net. This method is appropriate because the model considers two types of penalized regression- ridge and lasso -and the balance between them, as well as a linear model. This means that Elastic Net is very flexible; it can adapt to a lot of different relationships between variables. Similarly to a regression tree, Elastic Net can perform selection, so we are able to narrow down the features that actually make a significant difference on the weight of the flakes. However, Elastic Net also allows for greater distinction in predicted values than a regression tree, as with a regression tree many different values for the features end up getting lumped under one average response. 

## Section 5.2: Method

In this section, we will be using Elastic Net to predict the weight of the stone flakes. Elastic Net is a form of penalized regression. In linear regression, the model tries to reduce error, but in penalized regression such as Elastic Net, the model is trying to reduce error plus some other penalty term. The penalty term helps make sure that the coefficients of the model don't get too big, so that we don't get a model we can't be confident in. Since Elastic Net considers ridge regression, lasso regression, and everything in between, we need to consider all possibilities when creating our model. We also need to consider many possibilities for lambda, which is our penalty. A larger lambda means a bigger penalty, and so we choose a range from 0-25 as our options for lambda. For every balance between ridge and lasso, or alpha, a lambda is then chosen that best reduces the RMSE. 

\newpage

## Section 5.3: Results

\begin{table}[ht]
\centering
\begin{tabular}{rrr|rrr|rrrr}
  \hline
 Alpha & Lambda & RMSE & Alpha & Lambda & RMSE & Alpha & Lambda & RMSE \\ 
  \hline
  0.000 & 0.600 & 5.098 & 0.330 & 0.200 & 5.063 & 0.670 & 0.100 & 5.073 \\ 
  0.010 & 0.300 & 5.135 & 0.340 & 0.200 & 5.086 & 0.680 & 0.100 & 5.071 \\ 
  0.020 & 0.300 & 5.093 & 0.350 & 0.300 & 5.068 & 0.690 & 0.100 & 5.053 \\ 
  0.030 & 0.300 & 5.170 & 0.360 & 0.100 & 5.057 & 0.700 & 0.100 & 5.042 \\ 
  0.040 & 0.300 & 5.070 & 0.370 & 0.300 & 5.057 & 0.710 & 0.100 & 5.103 \\ 
  0.050 & 0.300 & 5.085 & 0.380 & 0.200 & 5.053 & 0.720 & 0.000 & 5.082 \\ 
  0.060 & 0.200 & 5.049 & 0.390 & 0.100 & 5.079 & 0.730 & 0.100 & 5.070 \\ 
  0.070 & 0.500 & 5.101 & 0.400 & 0.100 & 5.069 & 0.740 & 0.100 & 5.150 \\ 
  0.080 & 0.200 & 5.103 & 0.410 & 0.200 & 5.102 & 0.750 & 0.100 & 5.066 \\ 
  0.090 & 0.200 & 5.073 & 0.420 & 0.100 & 5.051 & 0.760 & 0.200 & 5.039 \\ 
  0.100 & 0.300 & 5.231 & 0.430 & 0.200 & 5.067 & 0.770 & 0.200 & 5.111 \\ 
  0.110 & 0.400 & 5.060 & 0.440 & 0.100 & 5.112 & 0.780 & 0.200 & 5.134 \\ 
  0.120 & 0.300 & 5.085 & 0.450 & 0.200 & 5.086 & 0.790 & 0.000 & 5.074 \\ 
  0.130 & 0.300 & 5.092 & 0.460 & 0.100 & 5.056 & 0.800 & 0.200 & 5.134 \\ 
  0.140 & 0.300 & 5.086 & 0.470 & 0.200 & 5.072 & 0.810 & 0.100 & 5.064 \\ 
  0.150 & 0.300 & 5.081 & 0.480 & 0.100 & 5.097 & 0.820 & 0.100 & 5.085 \\ 
  0.160 & 0.300 & 5.085 & 0.490 & 0.100 & 5.055 & 0.830 & 0.100 & 5.073 \\ 
  0.170 & 0.200 & 5.209 & 0.500 & 0.200 & 5.071 & 0.840 & 0.100 & 5.082 \\ 
  0.180 & 0.400 & 5.081 & 0.510 & 0.300 & 5.112 & 0.850 & 0.100 & 5.140 \\ 
  0.190 & 0.200 & 5.079 & 0.520 & 0.100 & 5.092 & 0.860 & 0.100 & 5.030 \\ 
  0.200 & 0.200 & 5.057 & 0.530 & 0.100 & 5.061 & 0.870 & 0.100 & 5.109 \\ 
  0.210 & 0.300 & 5.109 & 0.540 & 0.100 & 5.075 & 0.880 & 0.100 & 5.077 \\ 
  0.220 & 0.400 & 5.058 & 0.550 & 0.300 & 5.146 & 0.890 & 0.100 & 5.091 \\ 
  0.230 & 0.300 & 5.050 & 0.560 & 0.100 & 5.102 & 0.900 & 0.100 & 5.041 \\ 
  0.240 & 0.300 & 5.047 & 0.570 & 0.100 & 5.071 & 0.910 & 0.100 & 5.048 \\ 
  0.250 & 0.200 & 5.049 & 0.580 & 0.100 & 5.049 & 0.920 & 0.100 & 5.095 \\ 
  0.260 & 0.200 & 5.104 & 0.590 & 0.000 & 5.102 & 0.930 & 0.100 & 5.062 \\ 
  0.270 & 0.300 & 5.095 & 0.600 & 0.000 & 5.073 & 0.940 & 0.100 & 5.078 \\ 
  0.280 & 0.200 & 5.041 & 0.610 & 0.100 & 5.088 & 0.950 & 0.200 & 5.077 \\ 
  0.290 & 0.200 & 5.081 & 0.620 & 0.200 & 5.077 & 0.960 & 0.200 & 5.075 \\ 
  0.300 & 0.200 & 5.109 & 0.630 & 0.100 & 5.043 & 0.970 & 0.100 & 5.060 \\ 
  0.310 & 0.100 & 5.049 & 0.640 & 0.100 & 5.052 & 0.980 & 0.100 & 5.114 \\ 
  0.320 & 0.200 & 5.057 & 0.650 & 0.100 & 5.075 & 0.990 & 0.100 & 5.081 \\ 
  0.330 & 0.200 & 5.063 & 0.660 & 0.100 & 5.084 & 1.000 & 0.100 & 5.060 \\ 
   \hline
\end{tabular}
\end{table}

```{r}
# This line loads the package we need in order to run our Elastic Net model. 
suppressMessages(library(glmnet))
```

```{r}
# This line creates the design matrix, which is essentially the features we will be using to predict for Weight.
XD <- model.matrix(Weight ~ . , data = Lithics)
```

```{r}
# Since we are doing Elastic Net, which can be closer to ridge, lasso, or somewhere in between, we need to choose a range for our alpha level to account for this variability. We choose a range from 0 (which is used for ridge) to 1 (which is used for lasso).
alphaseq <- seq(from = 0, to =1 , by =.01)
# Then we need to create a data frame to hold our alpha values, the lambda that will be chosen for each alpha, and the RMSE resulting from running elastic net with both the alpha and lambda. This data frame will be saved as "storage" so it is easy to reference later. 
storage <- data.frame("Alpha" = rep(NA,length(alphaseq)), "Lambda" = rep(NA,length(alphaseq)), "RMSE" = rep(NA,length(alphaseq)))

# We are about to run a for loop, but first we set a variable "a" equal to 1. This will just make it easy to place our data in the correct row each time we run the loop. We start with one because the first time we run the loop, we want our numbers to go in row 1. 
a = 1 
# This line sets a random seed
set.seed(101)
# This line starts a for loop
for( i in alphaseq ){
  # This line of code runs a 10-fold CV for the best value of lambda for the elastic net. The model runs through all the options for lambda, and chooses the one that results in the smallest validation RMSE.
  # To run this model, We input our design matrix (minus the intercept column), our response variable, our alpha of i, and our sequence of lambdas we are choosing from, which is 0-25 in .1 increments. All of this is stored as "cv.out."
  cv.out <- cv.glmnet(XD[,-1], Lithics[,"Weight"], alpha = i,lambda = seq(from = 0, to = 25, by = .1))
  # Then, we take the smallest lambda determined by the model, and put it into the correct row and "Lambda" column of "storage." We then calculate the RMSE and store that in the RMSE column, and store the alpha we are on in our loop in that column. 
  storage$Lambda[a] <- cv.out$lambda.min
  storage$RMSE[a] <- sqrt(min(cv.out$cvm))
  storage$Alpha[a] <- i
  # Finally, we add one to a, so that the next time the loop runs everything will get stored in the row before the last. 
   a = a + 1 
}
# This loop will repeat 101 times.
```


```{r}
# This is the code used to create the table displaying the values of alpha, lambda and corresponding RMSEs.  In the function, we say that we want the first big column to contain one third of our data, or rows 1-34 of the alpha, lambda, and RMSEs. The second big column will have rows 34-67, and the third big columns will have rows 68-101 . The double occurrence of row 34 will later be deleted in the text of the column. We specify digits as 3 because we want to show more decimal places thna the default. 

#xtable(cbind(storage[1:34,1:3], storage[34:67,1:3], storage[68:101,1:3]), digits=3)
```


After running our Elastic Net for multiple alphas and lambdas, we found that the lowest RMSE occurred with an alpha of .86 and a lambda of .1. This means that Elastic Net is closer to lasso than ridge, and that the penalty is very small. The RMSE of 5.03 means that on average, our predictions for flake weight differ from the true flake weights by 5.03 grams. Looking at the table below, we can see the coefficients of the elastic net model that gave us this RMSE. 


```{r}
# This line of code trains our elastic net model using the alpha and lambda determined in the previous loop. We once against input our design matrix XD without the intercept, our response variable, the alpha, and lambda. 
elastic.final <- glmnet(XD[,-1], Lithics[,"Weight"], alpha =.86 ,
                      lambda = .1)

# This line of code stores the coefficients of the Elastic Net model undee the name "elastic.betas."
elastic.betas <- as.numeric(coefficients(elastic.final))

# This line of code creates a data frame called "Betas" that houses the coefficients of the elastic net model
Betas <- data.frame("Coefficients" = elastic.betas)

# This line of code assigns each row of "Betas" its correct name, which are taken from the columns of the design matrix. 
rownames(Betas) <- colnames(XD)
```

\newpage

```{r}
# This line of code formats the "Betas" data frame as a table.
knitr::kable(Betas)


```

# Conclusions

After using KNN, a regression tree, and Elastic Net to predict the weight of stone flakes, and assessing the predictive accuracy of all three techniques using the RMSE, we would recommend using KNN with 9 neighbors for this predictive task. KNN yielded an RMSE of 4.35, which is lower than the RMSE oif the regression tree, 5.94, and Elastic Net, which had an RMSE of 5.03. A low RMSE means that the model or algorithm does a good job predicting for the response, which is why we believe the algorithm with the lowest RMSE, which is KNN, should be chosen for this task.

Since the range of the flake weights is 99.53 grams, we believe that these methods are appropriate to use to predict the weight of the stone flakes. Over a range of nearly 100 grams, KNN being off by 4.35 grams is small. Thus, we feel comfortable recommending KNN as a suitable predictor for stone flake weight. In fact, any of the methods would be suitable predictors of stone flake weight, as they all have similar RMSEs. Depending on what a client wanted in a model or algorithm, any of the three choices of KNN, regression tree, or Elastic Net could be appropriate.

One potential limitation of this research is that none of the predictive methods are able to be used for other sites and data. Archaeological sites and the artifacts within them vary widely from site to site, and so methods used to predict the weight of stone flakes in this site will not be able to be used for flakes originating from any other area. 







